# tensorflow
TensorFlow学习

一、简介
使用 TensorFlow, 你必须明白 TensorFlow:
  使用图 (graph) 来表示计算任务.
  在被称之为 会话 (Session) 的上下文 (context) 中执行图.
  使用 tensor 表示数据
  通过 变量 (Variable) 维护状态.
  使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.
  
TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 简而言之，图是由op构成，然后op之间由Tensor进行流动交互。

一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是 tensorflow::Tensor 实例.

详细见http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html 

二、MNIST数据集训练

MNIST是在机器学习领域中的一个经典问题。该问题解决的是把28x28像素的灰度手写数字图片识别为相应的数字，其中数字的范围从0到9.

http://yann.lecun.com/exdb/mnist/  提供了训练集与测试集数据的下载。

train-images-idx3-ubyte.gz包含 训练集图片 - 55000 张 训练图片, 5000 张 验证图片

train-labels-idx1-ubyte.gz包含 训练集图片对应的数字标签

t10k-images-idx3-ubyte.gz包含  测试集图片 - 10000 张 图片

t10k-labels-idx1-ubyte.gz包含  测试集图片对应的数字标签


（1）首先选用简单的两层神经网络，训练分类成功率可以达到95%，代码见2.1


（2）在训练神经网络过程中，通常使用的几种防止过拟合方法

  1.引入正则化
  正则化的思想十分简单明了。由于模型过拟合极有可能是因为我们的模型过于复杂。因此，我们需要让我们的模型在训练的时候，在对损失函数进行最小化的同时，也需要让对参数添加限制，这个限制也就是正则化惩罚项。 
  假设我们模型的损失函数为 
  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))
  加入正则项 L 后，损失函数为 
  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction)+rL)
  常用的正则化有两种：L1正则与L2正则
 
  2.Dropout
   Dropout是在深度学习中降低过拟合风险的常见方法，它是由大牛Hinton提出的。Hinton认为在神经网络产生过拟合主要是因为神经元之间的协同作用产生的。因此在神经网络进行训练的时候，让部分神经元失活，这样就阻断了部分神经元之间的协同作用，从而强制要求一个神经元和随机挑选出的神经元共同进行工作，减轻了部分神经元之间的联合适应性。

  3.提前终止训练
  模型在验证集上的误差在一开始是随着训练集的误差的下降而下降的。当超过一定训练步数后，模型在训练集上的误差虽然还在下降，但是在验证集上的误差却不在下降了。此时我们的模型就过拟合了。因此我们可以观察我们训练模型在验证集上的误差，一旦当验证集的误差不再下降时，我们就可以提前终止我们训练的模型。

  4.增加样本量
  在实际的项目中，你会发现，上面讲述的那些技巧虽然都可以减轻过拟合的风险，但是却都比不上增加样本量来的更实在。为什么增加样本可以减轻过拟合的风险呢？这个要从过拟合是啥来说。过拟合可以理解为我们的模型对样本量学习的太好了，把一些样本的特殊的特征当做是所有样本都具有的特征。举个简单的例子，当我们模型去训练如何判断一个东西是不是叶子时，我们样本中叶子如果都是锯齿状的话，如果模型产生过拟合了，会认为叶子都是锯齿状的，而不是锯齿状的就不是叶子了。如果此时我们把不是锯齿状的叶子数据增加进来，此时我们的模型就不会再过拟合了。因此其实上述的那些技巧虽然有用，但是在实际项目中，你会发现，其实大多数情况都比不上增加样本数据来的实在.
  
  
  在代码2.2中加入了正则化和Dropout
  
  （3）使用卷积神经网络训练mnist
  
  卷积神经网络（Convolutional Neural Network，CNN或ConvNet）是一种
具有局部连接、权重共享等特性的深层前馈神经网络。
卷积神经网络最早是主要用来处理图像信息。如果用全连接前馈网络来处
理图像时，会存在以下两个问题：

参数太多：如果输入图像大小为100 × 100 × 3（即图像高度为100，宽
度为100，3个颜色通道：RGB）。在全连接前馈网络中，第一个隐藏层的每个神
经元到输入层都有100 × 100 × 3 = 30, 000个相互独立的连接，每个连接都对应
一个权重参数。随着隐藏层神经元数量的增多，参数的规模也会急剧增加。这
会导致整个神经网络的训练效率会非常低，也很容易出现过拟合。

局部不变性特征：自然图像中的物体都具有局部不变性特征，比如在
尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈网络很难提取
这些局部不变特征，一般需要进行数据增强来提高性能。

卷积神经网络是受生物学上感受野的机制而提出。感受野（receptive field）
主要是指听觉、视觉等神经系统中一些神经元的特性，即神经元只接受其所支
配的刺激区域内的信号。在视觉神经系统中，视觉皮层中的神经细胞的输出依
赖于视网膜上的光感受器。视网膜上的光感受器受刺激兴奋时，将神经冲动信
号传到视觉皮层，但不是所有视觉皮层中的神经元都会接受这些信号。一个神
经元的感受野是指视网膜上的特定区域，只有这个区域内的刺激才能够激活该
神经元。

  目前的卷积神经网络一般是由卷积层、池化层和全连接层交叉堆叠而成的
前馈神经网络，使用反向传播算法进行训练。卷积层进行特征提取，池化层主要作用是提供了很强的鲁棒性（例如max-pooling是取一小块区域中的最大值，此时若此区域中的其他值略有变化，或者图像稍有平移，pooling后的结果仍不变），并且减少了参数的数量，防止过拟合现象的发生。池化层一般没有参数，所以反向传播的时候，只需对输入参数求导，不需要进行权值更新。

 Caffe中卷积的实现十分巧妙，详细可以参考一下这篇论文  https://hal.inria.fr/file/index/docid/112631/filename/p1038112283956.pdf 

  cnn训练mnist代码见2.3
  
  （4）递归神经网络
  
递归神经网络（Recurrent Neural Networks，RNN）是两种人工神经网络的总称：时间递归神经网络（recurrent neural network）和结构递归神经网络（recursive neural network）。时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。

RNN一般指代时间递归神经网络。单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。


  RNN主要解决序列数据的处理，比如文本、语音、视频等等。这类数据的样本间存在顺序关系，每个样本和它之前的样本存在关联。比如说，在文本中，一个词和它前面的词是有关联的；在气象数据中，一天的气温和前几天的气温是有关联的。一组观察数据定义为一个序列，从分布中可以观察出多个序列。

参考博客 ： https://feisky.xyz/machine-learning/rnn/

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

http://karpathy.github.io/2015/05/21/rnn-effectiveness/

  lstm训练mnist代码见2.4

三、TensorBoard:可视化学习

TensorBoard 通过读取 TensorFlow 的事件文件来运行。TensorFlow 的事件文件包括了你会在 TensorFlow 运行中涉及到的主要数据。

下面是 TensorBoard 中汇总数据（Summary data）的大体生命周期。

首先，创建你想汇总数据的 TensorFlow 图，然后再选择你想在哪个节点进行汇总(summary)操作

比如，假设你正在训练一个卷积神经网络，用于识别 MNISt 标签。你可能希望记录学习速度(learning rate)的如何变化，以及目标函数如何变化。通过向节点附加scalar_summary操作来分别输出学习速度和期望误差。然后你可以给每个 scalary_summary 分配一个有意义的 标签，比如 'learning rate' 和 'loss function'。

或者你还希望显示一个特殊层中激活的分布，或者梯度权重的分布。可以通过分别附加 histogram_summary 运算来收集权重变量和梯度输出。

在TensorFlow中，所有的操作只有当你执行，或者另一个操作依赖于它的输出时才会运行。创建的这些节点（summary nodes）都围绕着你的图像：没有任何操作依赖于它们的结果。因此，为了生成汇总信息，我们需要运行所有这些节点。这样的手动工作是很乏味的，因此可以使用tf.merge_all_summaries来将他们合并为一个操作。

然后你可以执行合并命令，它会依据特点步骤将所有数据生成一个序列化的Summary protobuf对象。最后，为了将汇总数据写入磁盘，需要将汇总的protobuf对象传递给tf.train.Summarywriter
SummaryWriter 的构造函数中包含了参数 logdir。这个 logdir 非常重要，所有事件都会写到它所指的目录下。此外，SummaryWriter 中还包含了一个可选择的参数 GraphDef。如果输入了该参数，那么 TensorBoard 也会显示你的图像。

现在已经修改了你的图，也有了 SummaryWriter，现在就可以运行你的神经网络了！如果你愿意的话，你可以每一步执行一次合并汇总，这样你会得到一大堆训练数据。这很有可能超过了你想要的数据量。你也可以每一百步执行一次合并汇总，或者如下面代码里示范的这样。




